---
title: "2023-08-03 TIL"
date: 2023-08-03
---

# 2023-08-03 TIL

## Generative Model

- Generative model
  - 단순히 단어, 문장을 `생성` 해 내는 것이 전부는 아님
  - 가장 기본적인 목표는 data의 확률 분포 p를 알아서, 해당 분포를 이용해 데이터를 만들어 내는 것 (sampling)
  - Density Estimation: x가 강아지 같으면 p(x)가 높고, 아니면 p(x)가 낮음. Anomaly Detection에 유용
    - 입력이 주어졌을 때 확률 값을 얻어낼 수 있는 이런 모델을 `explicit model`이라고 함
    - 반대는 `implicit model`
  - p(x)를 어떻게 만들어야 하는지가 문제
  - n개의 binary pixel을 표현하기 위해 필요한 parameter의 수는 $2^n - 1$개!
  - n개의 픽셀이 서로 다 독립적인 경우, parameter는 n개만 있으면 됨!
    - 그러나 이렇게 모든 경우가 다 independent한 건 너무 말이 안 되는 가정
    - 표현할 수 있는 데이터의 수가 너무 적다!
- Conditional Independence
  - Chain Rule과 Conditional Independence를 잘 활용해 보자!
  - Markow assumption을 이용, i번째 값이 i - 1번째 값에만 의존한다고 가정
  - Parameter의 수는 $2n - 1$개가 됨!
  - 이런 류의 모델을 auto-regressive model이라고 함
- Auto-regressive Model
  - 하나의 정보가 이전 정보들에 dependent한 모델
  - i번째 정보가 i-1번째에만 의존적일 수도, 전체 값들에 대해 모두 의존적일 수도 있음
- NADE
  - i번째 정보가 이전의 모든 정보에 의존적인 모델
  - 매 입력 값 마다 번 입력 차원이 달라짐
  - Explicit model
- Pixel RNN: RNN을 이용해 auto-regressive model 구축

## Latent Variable Model

- Variational Inference
  - Posterior Distribution을 찾는 데 목적이 있음
  - 그러나 posterior를 찾는 게 너무 힘드므로, variational distribution으로 근사
  - 이 과정에서 KL Divergence를 줄이는 게 목표
  - 그러나 KL Divergence를 줄이는 방법은 현실적으로 없으므로, ELBO를 높이는 방법으로 KL Divergence를 줄여 보는 것
  - ELBO는 Reconstruction Term과 Prior Fitting Term으로 구성됨
    - Reconstruction Term: Auto encoder의 loss를 줄임
    - Prior Fitting Term: Latent distribution과 prior distribution 사이의 KL Divergence의 간격을 줄임
- VAE
  - VAE는 VI를 활용하는 모델. Latent space의 값을 Decoder 태워서 나오는 값으로 data를 생성
  - Autoencoder와는 좀 많이 다름
  - Intractable model로, likelihood를 계산하기 힘듦
  - KL Divergence를 사용하고, 계산의 편의성을 위해 prior를 isotropic 가우시안을 사용
    - KL Divergence가 closed form으로 나옴
  - Prior를 가우시안이 아닌 분포로 설정하려고 하는 경우, AAE 라는 방법을 사용
    - Prior에 GAN을 사용
- GAN
  - Generator와 Discriminator가 서로 경쟁하면서 학습
  - 진짜 데이터의 distribution과 학습하려 하는 generator 사이의 JSD를 최대한 줄이는 게 목표
  - DCGAN: 이미지 도메인에서 GAN 활용
  - Info-GAN: 학습할 때, c라는 추가 클래스 정보를 줌
  - Text2Image: 문장을 통해 이미지를 만듦
  - CycleGAN: cycle-consistency loss를 줄이는 방향으로 학습
  - Star-GAN: 이미지를 컨트롤 할 수 있게 변경
  - Progressive-GAN: 저차원의 이미에서 고차원 이미지로 천천히 늘려나가는 방식으로 학습
