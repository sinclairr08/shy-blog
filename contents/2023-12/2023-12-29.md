# AdamW

## AdamW

- `Decoupled Weight Decay Regularization` 이라는 논문에서 소개된 optimizer
- Adam의 변형으로, weight decay를 적용한 Adam
  - gradient descent를 적용할 때, 이전 weight를 그대로 사용하지 않고 특정 비율로 감소된 값을 사용
  - 즉, w_t = w\_(t-1) - gradinet 식에서 w\_(t-1) 앞에 1 미만의 계수를 붙이는 것
- Adam보다 좋은 성능을 많이 보여줌

## References

1. https://www.google.com/search?q=adamw&sourceid=chrome&ie=UTF-8
2. https://arxiv.org/abs/1711.05101
3. https://wikidocs.net/195019
