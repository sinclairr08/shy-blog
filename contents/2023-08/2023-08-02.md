# 2023-08-02 TIL

## CNN

- Convolution: 두 함수를 적당히 섞어주는 연산
- 한 개의 필터만 있는 것이 아닌, 여러 필터를 통해 여러 출력을 만들어 낼 수 있음
- Convolutional Layer, Pooling layer, FC layer 등으로 CNN은 구성이 됨
- FC layer는 줄이는 추세, 파라미터의 수가 너무 많기 때문
- Stride: 한 번에 필터를 옮기는 step의 크기
- Padding: 원래 데이터의 가장자리의 외곽에 값을 채워 넣음 이때 채워 넣는 값의 크기를 padding size라고 함
- 1 \* 1 convolution: 파라미터를 줄이는 효과가 있음

## Modern CNN

- AlexNet
  - GPU의 성능이 부족해서, 두 개의 GPU에서 나눠서 학습을 진행
  - 5개의 convolutional layert과 3개의 dense layer 사용
  - RELU activation function 사용, Gradient가 1 혹은 0이기 때문에 vanishing gradient problem 일부 해결
  - Data augmentation을 사용
  - Dropout 사용
- VGGNet
  - 3 \* 3 convolutional filter을 사용
  - 5 \* 5 하나와 3 \* 3 두개는 결과는 같지만 후자가 파라미터 수가 더 적음
- GoogLeNet
  - 1 \* 1 convolution을 활용해서 parameter를 줄임
  - 같은 크기지만 채널만 줄이는 방향으로 mapping 가능
  - 상기 두 개의 모델에 비해 parameter가 적음
- ResNet
  - Parameter의 수가 많으면, overfitting이 발생
  - Identity map (skip connection)을 추가, 학습이 더 잘 되는 현상을 확인
  - 입력과 출력의 차원이 다를 수 있으므로, 1 \* 1 colvolution 사용
  - 3 \* 3 앞단과 뒷단에 1 \* 1을 붙임. 3 \* 3 전에는 채널을 줄이고, 뒤에는 늘림
- DenseNet
  - ResNet에서 더하기 대신 concat 사용
  - 채널이 늘어나므로, 1 \* 1 convolution을 통해 채널을 줄임

## Computer Vision Applications

- Semantic Segmentation
  - 이미지의 특정 위치가 어떤 label 인지 분류, 어려운 문제
  - 자율 주행 등에 활용
- Fully Convolutinal Layer를 사용
  - Dense layer 대신 Colvolutional layer를 쓰면, 파라미터나 차원의 수는 똑같음
  - Input의 dimension에 독립적이기 때문에, semantic segementation에 유리
  - 이 과정을 convolutionalization이라고 하고, fully convolutional network라고 함
  - Deconvolution: Convolution의 역연산, 엄밀한 역은 아니지만, padding을 많이 줘서 그렇게 보이게 함
- Detection
  - 이미지 안에서 region을 뽑는 R-CNN 존재, Linear SVM 사용
  - SPPNet, Fast R-CNN, Faster R-CNN, YOLO: R-CNN 보다 빠른 속도로 detection을 가능하게 한 모델들

## RNN

- Sequential Model
  - 언어, 비디오 등 대부분의 데이터는 sequential data
  - 이런 sequential data의 경우에는 차원이라는 게 딱히 존재하지 않으므로, 기존 모델을 활용하기 힘듦
  - 입력이 여러 개 들어올 때, 다음 데이터를 예측하는 모델이 가장 일반적 (Language model)
  - Autoregressive model: 과거의 N개의 데이터만 보는 방법
    - Markov model: 과거의 1개의 데이터만 봄, 나의 현재는 바로 직전 과거에만 영향을 받음
  - Latent Autoregressive model: Hidden state를 이용, 이 hidden state가 과거의 정보를 요약
    - Latent를 어떻게 만드느냐도 중요
- Recurrent Neural Network
  - 자기 자신의 state와 이전의 latent 를 참고하는 모델
  - 이를 시간 순으로 풀게 되면, 아주 큰 fc layer와 동일함
  - Short-term dependency라는, 먼 과거의 정보를 고려하지 못 하는 문제가 존재
    - Vanishing / exploding gradient가 발생
    - tanh를 activation으로 사용하는 게 나음
- RNN의 문제를 해결하기 위해 더 발전된 LSTM 등장
  - Forget Gate: 어떤 정보를 지울 지 결정
  - Input Gate: 어떤 정보를 새롭게 쓸 지 결정
  - Update Cell: 정보를 어떻게 취합할 지
  - Output Gate: 취합된 정보를 바탕으로 어떻게 빼낼지
- Gate Recurrent Unit: 게이트가 2개인 RNN, LSTM보다 parameter가 적음에도 좋은 성능

## Attention Models

- Transformer는 recurrent하지 않은, attention을 다루는 sequential model
- Transformer과 attention은 NLP 문제에만 해당되지는 않음
- 기존 모델과 달리, 몇 개의 단어가 들어가든 재귀적으로 동작하지 않고, 한 번에 N 개의 단어에 대한 계산이 수행됨
- Attention 구조 설명
  1. 각각의 단어를 임베딩으로 변환
  2. 임베딩을 nn을 이용해 `Query`, `Key`, `Value`라는 벡터로 변환
  3. Key 와 Query의 내적으로 Score 스칼라를 만듦 (두 벡터는 차원이 같아야 함)
  4. 이 값들을 softmax를 통해 normalize
  5. softmax 값 \* value의 weighted sum을 통해 특정 단어의 encoding된 벡터
- N개의 단어가 주어지면, N^2의 computation power가 필요. 이 부분이 제약사항은 맞지만, 더 유연한 모델을 만들 수 있음
- Positional Encoding이 필요. 순서 정보가 없기 때문에, 순서 정보를 추가해 줘야 함
- DALL-E와 같은 모델에서도 사용함
