# Gradient Boosting

## Gradient Boosting

- Boosting에 Gradient Descent를 합친 방법
- Boosting 방법으로 weight가 아닌 함수를 근사
- 잔차를 줄이는 방향으로 학습하며, overfitting을 방지하기 위해 learning rate l을 둠
  - 잔차가 h(x)라면, F_m(x) = F_m-1(x) + lh_m(x) 가 되는 셈
- 이외에도 Gradient Boosting은 subsampling, early stopping, shrinking 등으로 overfitting을 방지
- LightGBM, XGBoost 등이 Gradient Boosting의 대표적인 모델 예시
