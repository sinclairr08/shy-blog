---
title: "Attention"
date: 2023-12-18
---

# Attention

## Attention

- Seq2Seq 모델은 멀리 떨어진 항목간의 관계가 잘 반영되지 못하는 Long-Term Dependency 문제 발생
- 어텐션 매커니즘은 input data에서 다른 데이터와의 상관관계를 반영한 데이터를 만드는 매커니즘
  - 이 **"데이터를 만든다"** 라는 부분을 어디서도 설명해주지 않아서 진짜 처음 이해할때 너무 힘듦
  - 결국 어텐션도 하나의 mlp다
- 입력으로 들어온 같은 문장 내에서 attention을 수행하는 것을 self-attention 이라고 함
  - 이 과정에서 query, key, value 이용

## References

1. https://velog.io/@jhbale11/%EC%96%B4%ED%85%90%EC%85%98-%EB%A7%A4%EC%BB%A4%EB%8B%88%EC%A6%98Attention-Mechanism%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80
2. https://engineering-ladder.tistory.com/73

## 기타

- 다시 쓰기