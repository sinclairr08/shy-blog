# LORA Tuning

## LORA Tuning

- Low-Rank Adaptaion의 약자로, LLM에 fine-tuning을 사용하기 위해 등장한 방법
- Dense layer를 row-rank 행렬로 decomposition 해서 학습
  - 이렇게 하면 full로 fine-tuning 하는 것에 비해 적은 parameter수, 및 메모리 소비량을 가지게 됨

## References

1. https://velog.io/@quasar529/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-LoRA-Low-Rank-Adaptation-of-Large-Language-Models
