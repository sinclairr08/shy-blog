# RLHF

## RLHF

- Reinforcement Learning from Human Feedback의 약자
- OpenAI에서 chatGPT를 fine-tuning할 때 사용한 방법
- 다음과 같이 3단계로 구성됨
  - Supervised Fine Tuning(SFT): 적은 양의 샘플 training set으로 fine-tuning
  - Reward Model: SFT 모델이 생성한 답변들을 인간 labeler들이 랭킹을 매김. 이를 이용해 reward model 학습
  - Proximal Policy Optimization(PPO): Reward model의 보상을 통해 SFT 모델을 마지막으로 학습
- 결국 인간이 개입해야 하고, 편향이 발생할 수 있다는 점은 단점

## References

1. https://moon-walker.medium.com/chatgpt%EC%97%90-%EC%A0%81%EC%9A%A9%EB%90%9C-rlhf-%EC%9D%B8%EA%B0%84-%ED%94%BC%EB%93%9C%EB%B0%B1-%EA%B8%B0%EB%B0%98-%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%9D%98-%EC%9B%90%EB%A6%AC-eb456c1b0a4a
