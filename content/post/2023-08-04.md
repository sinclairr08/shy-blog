# PCA

## PCA

- 데이터의 차원이 너무 크다면, 데이터의 의미를 제대로 표현하지 못함
- 데이터의 분산을 최대한 보존하며, 서로 직교하는 낮은 차원의 새로운 basis를 찾는 방법
  - 데이터가 의미있으려면, 각 값 별로 분산이 큰 방향을 찾는 것이 중요함
  - 이를테면 각 테이터들이 a라는 방향을 기준으로 값이 별로 차이나지 않는다면, a 방향으로 데이터를 분석하는 것은 의미가 적음
  - 반대로 b 방향으로는 값이 넓게 퍼져있는 경우(분산이 큰 경우), b 방향을 이용해 데이터의 특성을 분석하는 것이 유리
  - 즉, b 방향처럼 분산이 가장 커지는 방향의 벡터(`주성분`)를 basis로 삼으면 됨
- 이 basis로 span된 저차원 공간으로, 고차원의 데이터를 projection 하면 됨
- Basis를 구하기 위해서는 공분산행렬 $AA^T$에 대해 Eigen Decomposition을 수행해야 함
  - 가장 큰 eigen value를 가지는 eigen vector를 뽑으면 됨
- 차원은 줄이면서, 데이터의 특성은 최대한 보존할 수 있음

## References

1. https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/
